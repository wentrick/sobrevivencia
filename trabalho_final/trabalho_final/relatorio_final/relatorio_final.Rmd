---
title: "Aplicação de modelos sobrevivência mediante dados de insuficiência cardíaca"
author: "Davi Wentrick Feijó -200016806, Micael Egídio Papa da Silva - 211029236"
header_left: "Departamento de Estatistica"
header_right: "UNB"
date: \today
fontsize: 11pt
german: false # default is English
bibliography: bib/references.bib, bib/packages.bib
bibliographystyle: bib/bibstyle.bst
output: 
  UHHformats::pdf_simple:
    font: "Helvetica" # alternative: "TheSansUHH"  
---

```{r setup, include = FALSE}
# settings --> keep this chunk as it is!
knitr::opts_chunk$set(echo = FALSE, message = FALSE, 
  warning = FALSE, error = FALSE, cache = TRUE,
  fig.path='figs/', cache.path = 'cache/')
```

```{r load-packages, include = FALSE}
# packages
pacman::p_load(readr,tidyverse,survival,AdequacyModel,rms,knitr,xtable,kableExtra,ggplot2,ggfortify,ranger,BART,randomForestSRC,qpcR,lmtest,StepReg,ggcorplot)
```

```{r generate-package-refs, include=FALSE}
# Create a bib database for R packages used above
# NOTE: RUN THIS CODE CHUNK MANUALLY TO CREATE FILE BEFORE KNITTING
knitr::write_bib(
  x = c(.packages(), 'bookdown', 'rmarkdown', 'UHHformats',
    # Add here now all packages that are loaded above:
    'knitr', 'kableExtra', 'xtable', 'tidyverse'),
  file = 'bib/packages.bib'
)
```

<!-- This is how you can define comments in an .Rmd file (outside the R code snippets) -->

\newpage

# Introdução

A pesquisa e o estudo da insuficiência cardíaca representam um campo crucial na área da saúde, especialmente considerando o cenário contemporâneo. A insuficiência cardíaca é uma condição crônica debilitante que afeta milhões de pessoas globalmente, resultando em um ônus significativo para os sistemas de saúde e afetando diretamente a qualidade de vida dos pacientes. Neste contexto, a compreensão aprofundada dos mecanismos subjacentes, fatores de risco, estratégias de prevenção e tratamento é fundamental.

No mundo atual, onde as doenças cardiovasculares continuam a ser uma das principais causas de morbidade e mortalidade, a insuficiência cardíaca emerge como um desafio complexo e premente. A interseção entre fatores de risco modificáveis, como dieta, estilo de vida, poluição ambiental e condições socioeconômicas, tem um impacto direto na incidência e na progressão dessa condição cardíaca.

Além disso, a crescente longevidade da população e a prevalência de comorbidades relacionadas, como diabetes e hipertensão, têm contribuído para um aumento substancial na incidência de insuficiência cardíaca. Essa realidade destaca a importância crítica de investigar não apenas os aspectos biomédicos, mas também os contextos sociais, comportamentais e ambientais que desempenham um papel na manifestação e gestão dessa condição.

Os avanços na pesquisa, diagnóstico e terapia oferecem uma perspectiva promissora, mas ainda há lacunas significativas a serem preenchidas. A exploração contínua dos mecanismos moleculares, novas terapias farmacológicas, intervenções não farmacológicas e abordagens inovadoras de gerenciamento são áreas cruciais que exigem uma atenção contínua.

Portanto, compreender a insuficiência cardíaca não apenas como uma entidade clínica isolada, mas como um desafio multifacetado que requer abordagens interdisciplinares e holísticas, torna-se essencial. Esta compreensão abrangente é crucial para orientar políticas de saúde pública, estratégias de prevenção e intervenções clínicas mais eficazes, visando não apenas tratar, mas também mitigar os fatores de risco associados a essa condição.

Em resumo, o estudo da insuficiência cardíaca é um imperativo no panorama atual da saúde, exigindo uma abordagem abrangente e colaborativa para mitigar seu impacto, melhorar a qualidade de vida dos pacientes e aliviar a carga que essa condição exerce sobre os sistemas de saúde em todo o mundo. O presente trabalho busca estudar, dadas as devidas proporções, as causas do acréscimo e da recorrência de pacientes com insuficiência cardíaca por intermédio de uma modelagem estatística imbuída de metodologias de análises de sobrevivência em conjunto com modelos lineares generalizados. 

\newpage

# Metodologia


## Sobre o dataset

Doenças cardiovasculares (DCVs) são a principal causa de morte globalmente, tirando uma estimativa de 17,9 milhões de vidas a cada ano, o que representa 31\% de todas as mortes no mundo.

A insuficiência cardíaca é um evento comum causado por DCVs, e este conjunto de dados contém 12 características que podem ser usadas para prever a mortalidade por insuficiência cardíaca. Explicitando as características: 

Variáveis booleanas: 
\begin{itemize}
    \item Death event : 
Se o paciente faleceu durante o período de acompanhamento.
     \item Smoking : Se o paciente é fumante. 
     \item Sexo
     \item High blood pressure : Se o paciente tem hipertensão.
     \item Diabetes : Se o paciente tem diabetes.
     \item Anemia : Se o paciente tem anemia. 
\end{itemize}

Variáveis Numéricas:
\begin{itemize}
    \item Idade
    \item Creatinine phosphokinase: Nível da enzima CPK no sangue (mcg/L)
    \item ejection fraction :
Percentual de sangue deixando o coração a cada contração (porcentagem) 
    \item platelets : 
Plaquetas no sangue (quiloplaquetas/mL).
    \item serum creatinine : 
Nível de creatinina sérica no sangue (mg/dL).
    \item serum sodium : 
Nível de sódio sérico no sangue (mEq/L)
    \item time
\end{itemize}

A maioria das doenças cardiovasculares pode ser prevenida ao abordar fatores de risco comportamentais, como o uso de tabaco, dieta não saudável e obesidade, inatividade física e uso prejudicial de álcool, por meio de estratégias abrangentes para toda a população.

Pessoas com doenças cardiovasculares ou que estão em alto risco cardiovascular (devido à presença de um ou mais fatores de risco, como hipertensão, diabetes, hiperlipidemia ou doença já estabelecida) precisam de detecção precoce e manejo, onde um modelo de aprendizado de máquina pode ser de grande ajuda.

Dentre suas variáveis temos: 
- Idade
- Anemia : Diminuição de glóbulos vermelhos ou hemoglobina (variável booleana)
-    : Nível da enzima CPK no sangue (mcg/L)

## Função de sobrevivência - Log normal

A função de sobrevivência de uma distribuição log-normal descreve a probabilidade de uma variável aleatória contínua exceder um determinado valor ao longo do tempo. Na distribuição log-normal, os valores são logaritmicamente distribuídos, o que significa que o logaritmo dos dados segue uma distribuição normal.

Essa função é usada para modelar dados onde os valores têm uma distribuição assimétrica positiva e é útil em muitos contextos, como na análise de tempo de vida de produtos, estudos epidemiológicos ou financeiros. A função de sobrevivência da distribuição log-normal permite calcular a probabilidade de um evento ocorrer além de um determinado ponto no tempo, levando em consideração a natureza dos dados logarítmicos.



\[ S(t) = 1 - \Phi \left(\frac{\ln(t) - \mu}{\sigma}\right) \]

Onde:

\begin{enumerate}
    \item S(t)  é a função de sobrevivência em um tempo  t ,
    \item $\Phi$ é a função de distribuição acumulada da distribuição normal padrão,
    \item $\mu$ é a média da distribuição log-normal, 
    \item $\sigma$ é o desvio padrão da distribuição log-normal,
    \item $\ln(t)$  é o logaritmo natural de t, o tempo.
\end{enumerate}


## Kaplan-Meier

O método de Kaplan-Meier é uma técnica estatística usada para estimar a função de sobrevivência a partir de dados de tempo até um evento ocorrer. É frequentemente aplicado em estudos de sobrevivência ou análise de tempo até um evento (como tempo até a morte, falha de equipamentos, etc.).
 Funciona calculando as estimativas de probabilidade de sobrevivência em intervalos de tempo, ajustando os cálculos à medida que os eventos ocorrem ou os participantes são censurados. Essas estimativas são representadas graficamente na forma de uma curva de sobrevivência, que mostra a probabilidade de um indivíduo sobreviver além de um determinado ponto no tempo.
 Além disso, o método de Kaplan-Meier permite a comparação de diferentes grupos de indivíduos para avaliar se há diferenças significativas na função de sobrevivência entre eles. Isso pode ser feito usando testes estatísticos, como o teste log-rank, para determinar se as curvas de sobrevivência são estatisticamente diferentes entre os grupos.

A função Kaplan-Meier pode ser representada em LaTeX da seguinte maneira:

\[ \hat{S}(t) = \prod_{i:t_i \leq t} \left(1 - \frac{d_i}{n_i}\right) \]

Nesta fórmula:

\begin{enumerate}
\item-  $\hat{S}(t)$  é a estimativa da função de sobrevivência em um tempo  t .
\item- $t_{i}$  representa os tempos de eventos.
\item-  $d_{i}$  é o número de eventos no tempo   $t_{i}$  .
\item  $n_{i}$  é o número de indivíduos em risco no tempo  $t_{i}$ .
\end{enumerate}


## Função Hazard
 
A função hazard, na teoria da sobrevivência e análise de sobrevivência, descreve a taxa instantânea na qual um evento (como morte, falha de equipamento, etc.) ocorre em um determinado momento, dado que o indivíduo tenha sobrevivido até aquele ponto no tempo. É uma medida da probabilidade condicional de um evento ocorrer em um pequeno intervalo de tempo, dado que o indivíduo tenha sobrevivido até esse momento.

Matematicamente, a função hazard é definida como a razão entre a densidade de probabilidade de um evento ocorrer em um determinado ponto no tempo e a probabilidade de sobrevivência até esse ponto. Em um contexto contínuo, a função hazard é representada por  $\lambda \left( t \right)$ tal que : 

Claro, a função hazard em um contexto contínuo é frequentemente representada da seguinte maneira em LaTeX:

\[ \lambda(t) = \lim_{{\Delta t \to 0}} \frac{P(t \leq T < t + \Delta t \mid T \geq t)}{\Delta t} \]

Nesta fórmula:

\begin{enumerate}

\item-   $\lambda \left( t \right)$   é a função hazard no tempo t 
\item- $P(t \leq T < t + \Delta t \mid T \geq t) $  é a probabilidade condicional de um evento ocorrer no intervalo $[t, t + \Delta t]$ dado que o evento não ocorreu até o tempo t
\item-  $\Delta t $ representa um intervalo de tempo infinitesimalmente pequeno.

\end{enumerate}

Assim, conseguimos expressar a taxa de risco de um evento ocorrer em um tempo específico, dado que o indivíduo sobreviveu até aquele ponto no tempo.


A interpretação da função hazard é crucial. Se a função hazard é constante ao longo do tempo, isso indica que a taxa de risco do evento é constante, o que é característico de muitos processos naturais. Por outro lado, se a função hazard aumenta ou diminui ao longo do tempo, isso indica mudanças na taxa de risco ao longo do tempo.

Uma função hazard crescente sugere que o risco de um evento aumenta com o tempo, enquanto uma função hazard decrescente indica que o risco diminui ao longo do tempo. Por exemplo, em estudos médicos, a função hazard pode mostrar como o risco de certas condições de saúde, como doenças cardiovasculares ou câncer, pode variar ao longo da vida de um paciente.

## Análise dos resíduos - Cox Snell

Esses resíduos são uma maneira de avaliar a adequação do modelo ajustado aos dados e examinar como bem o modelo de riscos proporcionais está descrevendo a relação entre as variáveis explicativas e a taxa de risco.

Quando ajustamos um modelo de riscos proporcionais de Cox para dados de sobrevivência, estamos modelando como as variáveis independentes influenciam a taxa de risco (hazard) de um evento ocorrer ao longo do tempo. Os resíduos de Cox-Snell são calculados a partir da distribuição acumulada dos tempos observados versus a distribuição acumulada dos tempos esperados, conforme previstos pelo modelo.

Para calcular os resíduos de Cox-Snell, os tempos de sobrevivência observados são transformados usando as estimativas de probabilidade de sobrevivência derivadas do modelo ajustado. Em seguida, esses tempos transformados são comparados com uma distribuição teórica (normalmente uma distribuição exponencial se o modelo está bem especificado) para verificar se o modelo ajustado está adequado aos dados.

Se os resíduos de Cox-Snell se comportarem de maneira semelhante à distribuição teórica esperada (por exemplo, se seguirem uma distribuição exponencial), isso sugere que o modelo de riscos proporcionais está ajustando bem os dados observados. Por outro lado, desvios significativos dessa distribuição teórica podem indicar problemas na especificação do modelo ou falta de ajuste aos dados.

São dados por :

\[ RS_i = -\ln(1 - \hat{S}(t_i)) \]

Onde: 
\begin{enumerate}
    \item $RS_{i}$  é o resíduo de Cox-Snell para o evento i.
    \item $\hat{S}t_{i}$  é a estimativa da função de sobrevivência no tempo $t_{i}$.
    
\end{enumerate}

Explicitamos assim que eles são uma medida da discrepância entre a probabilidade prevista de sobrevivência e a não ocorrência do evento até o tempo $t_{i}$, transformada para facilitar a avaliação do ajuste do modelo de riscos proporcionais de Cox aos dados de sobrevivência. 



## Seleção das variáveis

### Stepwise
O método stepwise é uma técnica usada na seleção de variáveis em modelos estatísticos, especialmente em modelos de regressão. Ele envolve a inclusão e exclusão iterativa de variáveis explicativas com base em critérios específicos, como a melhoria do ajuste do modelo ou a minimização do erro.

Existem duas formas principais de stepwise: forward stepwise e backward stepwise.

1. Forward Stepwise Selection: \\
   - Começa sem variáveis no modelo. \\
   - Iterativamente, adiciona variáveis uma por uma, escolhendo aquela que mais melhora o modelo com base em algum critério (como R², AIC, BIC, etc.). \\
   - Continua adicionando variáveis até que a adição de outras não melhore significativamente o modelo. \\

2. Backward Stepwise Selection:\\
   - Começa com todas as variáveis no modelo. \\
   - Iterativamente, remove variáveis uma por uma, escolhendo a que menos prejudica o modelo com base em algum critério. \\
   - Continua removendo variáveis até que a exclusão de outras não melhore significativamente o modelo. \\

Ambos os métodos podem ser combinados em um processo "stepwise" que realiza adições e remoções de variáveis no modelo até que não haja mais melhoria substancial nos critérios de avaliação.

Esse processo Stepwise é uma abordagem que combina os métodos forward e backward em um único procedimento. Ele realiza tanto adições quanto remoções de variáveis em cada iteração, considerando diferentes conjuntos de variáveis candidatas.

O processo do stepwise conjunto pode ser realizado da seguinte maneira: \\

1. Inicialização: Começa com um conjunto vazio de variáveis no modelo (forward) e todas as variáveis no modelo (backward). \\

2. Passos Iterativos:\\ 
   - Forward Step: Adiciona uma variável ao modelo se melhorar o critério de seleção (como R², AIC, BIC, etc.). \\
   - Backward Step: Remove uma variável do modelo se sua exclusão melhorar o critério de seleção.\\
   
3. Critério de Parada: O algoritmo continua alternando entre os passos forward e backward até que não seja possível adicionar mais variáveis que melhorem o critério de seleção (forward) ou remover mais variáveis que melhorem o critério (backward). Pode ser determinado por critérios como estabilidade do modelo, não melhoria substancial do critério de avaliação, ou outros critérios pré-definidos. \\

Essa abordagem combina as vantagens do forward e backward stepwise, permitindo a exploração de diferentes conjuntos de variáveis e a busca por um modelo que se ajuste bem aos dados. No entanto, também enfrenta críticas semelhantes às dos métodos individuais, como o potencial de overfitting e a sensibilidade aos critérios de seleção. \\

É importante ter em mente que, embora o método stepwise conjunto seja uma tentativa de abordar as limitações dos métodos forward e backward, a seleção de variáveis em modelos estatísticos geralmente requer uma compreensão cuidadosa do contexto do problema e a consideração de múltiplos métodos para validar os resultados. \\

 Onde, a abordagem stepwise não possui uma fórmula única, mas podemos exemplificar como a adição e remoção de variáveis podem ser representadas em um contexto de regressão linear múltipla.

Suponha um modelo de regressão linear múltipla onde estamos selecionando variáveis com base em algum critério de ajuste, como o AIC (Critério de Informação de Akaike):

A fórmula para o AIC é: \\ 
\[ AIC = 2k - 2\ln(\hat{L}) \]
 
Onde: \\
- \( k \) é o número de parâmetros no modelo. \\
- \( \hat{L} \) é a função de verossimilhança máxima do modelo. \\

No processo stepwise, durante a adição de variáveis, você pode comparar dois modelos:

1. Modelo com uma variável a mais: \\
   - \( AIC_1 = 2k_1 - 2\ln(\hat{L}_1) \) \\

2. Modelo atual: \\
   - \( AIC_0 = 2k_0 - 2\ln(\hat{L}_0) \) \\

Se \( AIC_1 < AIC_0 \), a variável adicional é considerada benéfica e é adicionada ao modelo.

Durante a remoção de variáveis, você faz o processo inverso:

1. Modelo sem uma variável: \\
   - \( AIC_2 = 2k_2 - 2\ln(\hat{L}_2) \) \\

2. Modelo atual: \\
   - \( AIC_0 = 2k_0 - 2\ln(\hat{L}_0) \) \\

Se \( AIC_2 < AIC_0 \), a variável é removida do modelo.

Esses são os princípios básicos do processo stepwise em termos de comparação de critérios para adição e remoção de variáveis. No entanto, a implementação exata pode variar dependendo do critério de seleção, do tipo de modelo e de outros fatores específicos ao contexto do problema.

## Escolha do modelo

### TRV
A estatística de razão de verossimilhança (likelihood ratio test) é uma ferramenta fundamental na comparação de modelos em análise de sobrevivência, especialmente quando se trabalha com modelos de riscos proporcionais de Cox.

Ela compara a adequação de dois modelos distintos, geralmente um modelo completo (mais complexo) e um modelo reduzido (menos complexo). A diferença na verossimilhança entre esses dois modelos é usada para avaliar se o modelo mais complexo oferece um ajuste significativamente melhor em comparação com o modelo mais simples.

A ideia central é comparar as verossimilhanças dos dois modelos (o modelo completo e o modelo reduzido) para determinar se a inclusão de variáveis adicionais ou complexidade no modelo completo melhora significativamente a capacidade do modelo de explicar os dados observados.

A estatística de razão de verossimilhança é calculada como o logaritmo natural da razão entre as verossimilhanças dos dois modelos. Em um contexto de riscos proporcionais de Cox, essa estatística segue aproximadamente uma distribuição qui-quadrado, assumindo que o modelo mais simples é verdadeiro (ou seja, não há diferenças reais entre os modelos).

Se a estatística de razão de verossimilhança for grande o suficiente, ou seja, se a diferença entre os modelos for significativa, isso indica que o modelo mais complexo se ajusta significativamente melhor aos dados do que o modelo mais simples. Portanto, pode-se rejeitar a hipótese nula de que o modelo mais simples é suficiente para descrever os dados.

Em resumo, a estatística de razão de verossimilhança é uma ferramenta estatística poderosa para comparar a adequação de modelos distintos na análise de sobrevivência, permitindo determinar se a inclusão de variáveis ou complexidade adicional resulta em uma melhoria significativa na capacidade do modelo de explicar os dados observados.


\[ \text{LR} = -2 \times (\ln(\mathcal{L}_{\text{reduzido}}) - \ln(\mathcal{L}_{\text{completo}})) \]

Onde :

\begin{enumerate}
    \item $\text{LR}$  é a estatística de razão de verossimilhança.
    \item $\ln(\mathcal{L}_{\text{reduzido}})$  é o logaritmo da verossimilhança do modelo reduzido.
    \item $\ln(\mathcal{L}_{\text{completo}})$  é o logaritmo da verossimilhança do modelo completo.
\end{enumerate}


Representando assim a diferença entre os logaritmos das verossimilhanças dos modelos completo e reduzido, multiplicada por -2 para ajustar a distribuição da estatística de razão de verossimilhança para uma distribuição qui-quadrado, que é usada para testar a significância estatística da diferença entre os modelos.


### BIC


O BIC é derivado da teoria da informação e é utilizado para comparar diferentes modelos com base na verossimilhança dos dados e no número de parâmetros do modelo. A ideia central é penalizar modelos mais complexos, aqueles com mais parâmetros, com o intuito de evitar o overfitting, ou seja, evitar que o modelo se ajuste excessivamente aos dados de treinamento e perca capacidade de generalização para novos dados.

A fórmula do BIC é dada por:

\[ BIC = -2 \times \ln(L) + k \times \ln(n) \]

Onde: 

\begin{enumerate}
    \item $\ln(L)$  é o logaritmo da verossimilhança do modelo, ou seja, o valor máximo da função de verossimilhança atingido pelo modelo.
    \item k  é o número de parâmetros no modelo.
    \item n é o número de observações nos dados.
\end{enumerate}

O BIC penaliza modelos mais complexos (com um número maior de parâmetros) adicionando um termo proporcional a \( k \times \ln(n) \) ao valor \( -2 \times \ln(L) \). Isso significa que, à medida que o número de parâmetros aumenta, o BIC aumenta, mas a penalização é maior para conjuntos de dados menores, refletida pelo termo \( \ln(n) \).

Ao comparar modelos, o BIC indica que o modelo com o valor mais baixo é preferível, pois alcança um bom ajuste aos dados, mas também é mais parcimonioso, evitando o sobreajuste. Portanto, o BIC é útil para a seleção de modelos, ajudando a encontrar um equilíbrio entre a capacidade de ajuste e a complexidade do modelo.

### AIC

O AIC é baseado na ideia de encontrar um equilíbrio entre a capacidade de ajuste do modelo aos dados e a complexidade do modelo, penalizando modelos mais complexos. Ele leva em consideração tanto a habilidade do modelo em ajustar os dados quanto o número de parâmetros utilizados, buscando encontrar o modelo que melhor se ajuste aos dados sem ser excessivamente complexo. Sendo dado por:

\[ AIC = -2 \times \ln(L) + 2k \]

Onde: 
\begin{enumerate}
    \item $\ln(L)$  é o logaritmo da verossimilhança do modelo.
    \item k  é o número de parâmetros no modelo.
\end{enumerate}


O AIC penaliza modelos mais complexos adicionando \( 2k \) ao valor \( -2 \times \ln(L) \), onde \( k \) representa o número de parâmetros no modelo. Portanto, à medida que o número de parâmetros aumenta, o AIC aumenta, mas ele também recompensa modelos com uma verossimilhança maior, refletida pelo termo \( -2 \times \ln(L) \).

Ao comparar modelos, o AIC indica que o modelo com o valor mais baixo é preferível, pois alcança um bom ajuste aos dados, mas também é mais parcimonioso, evitando o sobreajuste. 


### AICc

O AIC corrigido (ou AICc) é uma versão modificada do Critério de Akaike (AIC), especialmente útil em situações em que o tamanho da amostra é pequeno em relação ao número de parâmetros do modelo. Ele ajusta o AIC para levar em consideração a amostra limitada, oferecendo uma penalização mais forte para modelos mais complexos em comparação com o AIC padrão.

O AICc adiciona um fator de correção à penalidade do AIC, levando em conta o tamanho da amostra (\(n\)) e o número de parâmetros (\(k\)) no modelo. A fórmula do AICc é:

\[ AICc = AIC + \frac{2k(k+1)}{n - k - 1} \]

Aqui, além da penalidade padrão do AIC (\(2k\)), adiciona-se o termo \( \frac{2k(k+1)}{n - k - 1} \) como correção, onde \(n\) é o número de observações no conjunto de dados.

O AICc é particularmente valioso em conjuntos de dados pequenos, onde o AIC padrão pode superestimar a complexidade do modelo devido à amostra limitada. Ele oferece uma penalização adicional para modelos mais complexos, ajudando na seleção de modelos quando o tamanho da amostra é pequeno em relação ao número de parâmetros do modelo.


\newpage

# Resultados

## Analise Exploratória 


Nessa etapa, vamos estudar a distribuição das variáveis numéricas e categóricas do banco, com o objetivo de verificar sua distribuição e correlação.


```{r}
dados <- read_csv("heart_failure_clinical_records_dataset.csv") %>%
  mutate(censura = DEATH_EVENT,
         tempo = time,
         age = round(age,0)) %>% 
  select(-c(DEATH_EVENT,time))

head(dados)
```

### Correlação 

```{r}
cor_data=cor(dados %>% dplyr::select(-c(censura,tempo)))

ggcorrplot::ggcorrplot(cor_data, hc.order = TRUE, type = "lower",
   lab = TRUE,
   digits = 1)
```
Nenhuma das variáveis exibe uma correlação muito forte, sendo a correlação entre as variáveis `sex` e `smoking` a que mais se diferencia das demais.

### Análise das variáveis numéricas.

```{r}
ggplot(data=dados, aes(x=age)) +
  geom_histogram(fill="steelblue", color="black") +
  ggtitle("Histograma de Idade (Age)")

ggplot(data=dados, aes(x=creatinine_phosphokinase)) +
  geom_histogram(fill="steelblue", color="black") +
  ggtitle("Histograma de Creatinine Phosphokinase")

ggplot(data=dados, aes(x=ejection_fraction)) +
  geom_histogram(fill="steelblue", color="black") +
  ggtitle("Histograma de Ejection Fraction")

ggplot(data=dados, aes(x=platelets)) +
  geom_histogram(fill="steelblue", color="black") +
  ggtitle("Histograma de Platelets")

ggplot(data=dados, aes(x=serum_creatinine)) +
  geom_histogram(fill="steelblue", color="black") +
  ggtitle("Histograma de Serum Creatinine")

ggplot(data=dados, aes(x=serum_sodium)) +
  geom_histogram(fill="steelblue", color="black") +
  ggtitle("Histograma de Serum Sodium")
```

\newpage

### Modelo de sobrevivência não paramétrico de Kaplan-Meier.

```{r}
KM = survfit(Surv(dados$tempo,dados$censura)~1)

autoplot(KM)
```

\newpage

### A função de risco acumulado

```{r, fig.align = 'center'}
autoplot(KM,fun = "cumhaz",xlab = "Tempo",ylab = "Risco Acumulado H(t)")
```

\newpage

### Curva TTT (Tempo Total sobre Teste)

Gráfico do Tempo Total sobre Teste
```{r, fig.align = 'center'}
#curva TTT
TTT(dados$tempo)
```

Pelo formato da curva do gráfico, podemos notar que a função taxa de falha é monotonicamente crescente.

\newpage

### Analise das Variaveis Categoricas

#### Variavel Sex 

Vamos comparar as curvas de sobrevivência divididas por sexo, com o objetivo de verificar se essa variável influencia na curva de sobrevivência. Em seguida, iremos realizar um teste para verificar a diferença entre as curvas.

```{r, fig.align = 'center'}
KM = survfit(Surv(dados$tempo,dados$censura)~dados$sex)

#plot(KM,conf.int = F, mark.time = T, col = c("red","blue","green"))

autoplot(KM)
```

```{r}
#teste para diferenca de curvas
survdiff(Surv(tempo, censura) ~ sex, data=dados, rho = 1)
```

Podemos notar, tanto pelo gráfico quanto pelo teste com p-valor igual a 0.9, que a variável Sexo não parece influenciar nas curvas de sobrevivência.
\newpage

#### Variavel Diabetes

```{r, fig.align = 'center'}
# Resposta x diabetes

KM = survfit(Surv(dados$tempo,dados$censura)~dados$diabetes)

#plot(KM,conf.int = F, mark.time = T, col = c("red","blue","green"))
autoplot(KM)
```


```{r}
#teste para diferenca de curvas
survdiff(Surv(tempo, censura) ~ diabetes, data=dados, rho = 1)
```

Com um p-valor de 0.9, podemos concluir que não há evidência estatística suficiente para afirmar a existência de diferença significativa entre as curvas.

\newpage

#### Variavel Anaemia



```{r, fig.align = 'center'}
# Resposta x anaemia

KM = survfit(Surv(dados$tempo,dados$censura)~dados$diabetes)

#plot(KM,conf.int = F, mark.time = T, col = c("red","blue","green"))
autoplot(KM)
```


```{r}
#teste para diferenca de curvas
survdiff(Surv(tempo, censura) ~ diabetes, data=dados, rho = 1)
```

Com um p-valor de 0.9, podemos inferir que não há diferença estatisticamente significativa entre as categorias.

\newpage

#### Variavel High Blood Pressure


```{r, fig.align = 'center'}
# Resposta x blood pressure

KM = survfit(Surv(dados$tempo,dados$censura)~dados$high_blood_pressure)

#plot(KM,conf.int = F, mark.time = T, col = c("red","blue","green"))
autoplot(KM)
```

```{r}
#teste para diferenca de curvas
survdiff(Surv(tempo, censura) ~ high_blood_pressure, data=dados, rho = 1)
```

\newpage

#### Variavel Smoking

```{r, fig.align = 'center'}
# Resposta x smoking

KM = survfit(Surv(dados$tempo,dados$censura)~dados$smoking)

#plot(KM,conf.int = F, mark.time = T, col = c("red","blue","green"))
autoplot(KM)
```

```{r}
#teste para diferenca de curvas
survdiff(Surv(tempo, censura) ~ smoking, data=dados, rho = 1)
```


\newpage

## Seleção da distribuição. 

Vamos ajustar diferentes distribuições sobre o gráfico de Kaplan-Meier para selecionar aquela que melhor se adapta à curva. Além disso, estaremos verificando os valores de AIC, AIC corrigido e BIC para decidir a distribuição a ser utilizada.

```{r}
#create a Surv object
s <- with(dados,Surv(tempo,censura))
## Kaplan-Meier estimator without grouping
km.null <- survfit(data = dados, s ~ 1)
plot(km.null, ylim = c(0.5, 1),conf.int = F)

## Parametric estimation with Weibull distribution
weibull.null <- survreg(data = dados, s ~ 1, dist = "weibull")
lines(x = predict(weibull.null, type = "quantile", p = seq(0.01, 0.99, by=.01))[1,],
      y = rev(seq(0.01, 0.99, by = 0.01)),
      col = "red")

## Parametric estimation with log-logistic distribution
loglogistic.null <- survreg(data = dados, s ~ 1, dist = "loglogistic")
lines(x = predict(loglogistic.null, type = "quantile", p = seq(0.01, 0.99, by=.01))[1,],
      y = rev(seq(0.01, 0.99, by = 0.01)),
      col = "blue")

## Parametric estimation with log-normal distribution
lognormal.null <- survreg(data = lung, s ~ 1, dist = "lognorm")
lines(x = predict(loglogistic.null, type = "quantile", p = seq(0.01, 0.99, by=.01))[1,],
      y = rev(seq(0.01, 0.99, by = 0.01)),
      col = "green")

## Add legends
legend(x = "bottomleft",
       legend = c("Kaplan-Meier", "Weibull", "log-logistic","log-normal"),
       lwd = 2, bty = "n",
       col = c("black", "red", "blue","green"))
```


```{r}
#a funcao suvreg com weibull retorna o valor extremo, para isso temos que fazer uma transformacao para encontrar os parametrod da weibull padrao.
n = length(dados$tempo) #n de obs

gama_weibull = 1/weibull.null$scale

alpha_weibull = exp(weibull.null$icoef[1])


pws = 2 #numero de parametros da distribuicao

AICws = (-2*weibull.null$loglik[1])+(2*pws)

AICcws = AICws + ((2*pws*(pws+1))/(n-pws-1))

BICws = (-2*weibull.null$loglik[1])+(pws*log(n))

medidasw = cbind(AICws,AICcws,BICws)
```

#### Distribuicao Weibull

```{r}
cat("Weibull ~(",gama_weibull,",",alpha_weibull,")")

medidasw

summary(weibull.null)
```


```{r}
sigma_lognormal = lognormal.null$scale

mi_lognormal = lognormal.null$icoef[1]


plns = 2 #descobrir o que é isso

AIClns = (-2*lognormal.null$loglik[1])+(2*plns)

AICclns = AIClns + ((2*plns*(plns+1))/(n-plns-1))

BIClns = (-2*lognormal.null$loglik[1])+(plns*log(n))

medidalns = cbind(AIClns,AICclns,BIClns)
```

#### Distribuicao Log-Normal

```{r}
cat("Log-Normal ~(",mi_lognormal,",",sigma_lognormal,")")

medidalns

summary(loglogistic.null)
```


```{r}
gama_loglogistica = 1/loglogistic.null$scale

alpha_loglogistica = exp(loglogistic.null$icoef[1])


plls = 2 #descobrir o que é isso

AIClls = (-2*loglogistic.null$loglik[1])+(2*plls)

AICclls = AIClls + ((2*plls*(plls+1))/(n-plls-1))

BIClls = (-2*loglogistic.null$loglik[1])+(plls*log(n))

medidalls = cbind(AIClls,AICclls,BIClls)
```

#### Distribuicao Log-Logistica

```{r}
cat("Log-Logistica ~(",gama_loglogistica,",",alpha_loglogistica,")")

medidalls

summary(loglogistic.null)
```

\newpage

#### Comparando as 3 distribuicoes 

```{r}
medidasw
medidalns
medidalls
```

Podemos observar pelos valores de AIC, AICc e BIC que a distribuição mais recomendada é a log-normal, no entanto, todas estão bastante próximas, indicando que também poderiam ser utilizadas com resultados semelhantes.

\newpage

## Selecao de variaveis

### Manual

Vamos manualmente selecionar as variáveis que devem permanecer no modelo final. Isso será feito para posterior comparação com outro modelo selecionado por uma função já implementada no R. Ao final, pretendemos comparar os dois modelos.

#### Etapa 1: Ajustar os modelos com somente uma covariavel para verificar se sao significativas individualmente.


```{r}
lognormal.1 = survreg(data = dados, s ~ age, dist = "lognorm")
summary(lognormal.1)
```

```{r}
lognormal.2 = survreg(data = dados, s ~ anaemia, dist = "lognorm")
summary(lognormal.2)
```

```{r}
lognormal.3 = survreg(data = dados, s ~ creatinine_phosphokinase, dist = "lognorm")
summary(lognormal.3)
```

```{r}
lognormal.4 = survreg(data = dados, s ~ diabetes, dist = "lognorm")
summary(lognormal.4)
```

```{r}
lognormal.5 = survreg(data = dados, s ~ ejection_fraction  , dist = "lognorm")
summary(lognormal.5)
```

```{r}
lognormal.6 = survreg(data = dados, s ~ high_blood_pressure, dist = "lognorm")
summary(lognormal.6)
```

```{r}
lognormal.7 = survreg(data = dados, s ~ platelets, dist = "lognorm")
summary(lognormal.7)
```

```{r}
lognormal.8 = survreg(data = dados, s ~ serum_creatinine, dist = "lognorm")
summary(lognormal.8)
```

```{r}
lognormal.9 = survreg(data = dados, s ~ serum_sodium, dist = "lognorm")
summary(lognormal.9)
```

```{r}
lognormal.10 = survreg(data = dados, s ~ sex, dist = "lognorm")
summary(lognormal.10)
```

```{r}
lognormal.11 = survreg(data = dados, s ~ smoking, dist = "lognorm")
summary(lognormal.11)
```

Observando os p-valores de cada modelo ajustado com uma variavel sao significantes a nivel de 5%: `serum_sodium`,`serum_creatinine`,`high_blood_pressure`,`ejection_fraction` e `age`. 

\newpage

#### Etapa 2: Ajustar um modelo com todas essas variaveis significativas e comparar com modelos reduzidos removendo uma dessas variaveis.

```{r}
lognormal.12 = survreg(data = dados, s ~ age+ejection_fraction+high_blood_pressure+serum_creatinine+serum_sodium, dist = "lognorm")
```

```{r}
lognormal.13 = survreg(data = dados, s ~ age+ejection_fraction+high_blood_pressure+serum_creatinine, dist = "lognorm")
```

```{r}
lrtest(lognormal.12,lognormal.13)
```

```{r}
lognormal.14 = survreg(data = dados, s ~ age+ejection_fraction+high_blood_pressure+serum_sodium, dist = "lognorm")
```

```{r}
lrtest(lognormal.12,lognormal.14)
```

```{r}
lognormal.15 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+serum_sodium, dist = "lognorm")
```

```{r}
lrtest(lognormal.12,lognormal.15)
```

```{r}
lognormal.16 = survreg(data = dados, s ~ age+high_blood_pressure+serum_creatinine+serum_sodium, dist = "lognorm")
```

```{r}
lrtest(lognormal.12,lognormal.16)
```

```{r}
lognormal.17 = survreg(data = dados, s ~ ejection_fraction+high_blood_pressure+serum_creatinine+serum_sodium, dist = "lognorm")
```

```{r}
lrtest(lognormal.12,lognormal.17)
```

A única variável que não apresentou significância foi `high_blood_pressure`. Optamos por selecionar o modelo sem ela. Em seguida, deveríamos ajustar um modelo com as variáveis restantes e compará-lo com o modelo que excluiu a variável, para verificar se a exclusão conjunta tem algum impacto. No entanto, como apenas uma variável foi removida, não é necessário realizar esse procedimento, pois repetiríamos o teste já conduzido.

\newpage

#### Etapa 3: Ajustar o modelo sem a variável excluída e compará-lo com o modelo que continha as variáveis removidas na etapa 1.

O objetivo desta etapa é verificar se alguma das variáveis excluídas na etapa 1 (`anaemia`,`creatinine_phosphokinase`,`diabetes`,`platelets`,`sex`,`smoking`) é significativa em conjunto com as outras selecionadas na etapa 2.

```{r}
lognormal.18 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+serum_sodium, dist = "lognorm")
```

```{r}
lognormal.19 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+serum_sodium+anaemia, dist = "lognorm")
```

```{r}
lrtest(lognormal.18,lognormal.19)
```

```{r}
lognormal.20 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+serum_sodium+creatinine_phosphokinase, dist = "lognorm")
```

```{r}
lrtest(lognormal.18,lognormal.20)
```

```{r}
lognormal.21 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+serum_sodium+diabetes, dist = "lognorm")
```

```{r}
lrtest(lognormal.18,lognormal.21)
```

```{r}
lognormal.22 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+serum_sodium+platelets, dist = "lognorm")
```

```{r}
lrtest(lognormal.18,lognormal.22)
```

```{r}
lognormal.23 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+serum_sodium+sex, dist = "lognorm")
```

```{r}
lrtest(lognormal.18,lognormal.23)
```

```{r}
lognormal.24 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+serum_sodium+smoking, dist = "lognorm")
```

```{r}
lrtest(lognormal.18,lognormal.24)
```

Nenhuma das variáveis que foram inicialmente retiradas permanece no modelo com um nível de significância de 5%.

\newpage

#### Etapa 4: Vamos verificar se é possível simplificar o modelo obtido na etapa 3 removendo alguma variável.

```{r}
lognormal.25 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine, dist = "lognorm")
```

```{r}
lrtest(lognormal.18,lognormal.25)
```

```{r}
lognormal.26 = survreg(data = dados, s ~ age+ejection_fraction+serum_sodium, dist = "lognorm")
```

```{r}
lrtest(lognormal.18,lognormal.26) 
```

```{r}
lognormal.27 = survreg(data = dados, s ~ age+serum_creatinine+serum_sodium, dist = "lognorm")
```

```{r}
lrtest(lognormal.18,lognormal.27) 
```

```{r}
lognormal.28 = survreg(data = dados, s ~ ejection_fraction+serum_creatinine+serum_sodium, dist = "lognorm")
```

```{r}
lrtest(lognormal.18,lognormal.28) 
```

Com base nos resultados a um nível de significância de 5%, optamos por remover a variável `serum_sodium`. Com essa decisão, chegamos ao nosso modelo final para os efeitos principais.

\newpage

#### Etapa 5: Vamos verificar a interação entre as variáveis restantes no modelo final obtido na etapa anterior.

```{r}
lognormal.28 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine, dist = "lognorm")
```

```{r}
lognormal.29 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+(age*ejection_fraction), dist = "lognorm")
```

```{r}
lrtest(lognormal.28,lognormal.29) 
```

```{r}
lognormal.30 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+(age*serum_creatinine), dist = "lognorm")
```

```{r}
lrtest(lognormal.28,lognormal.30) 
```

```{r}
lognormal.31 = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+(ejection_fraction*serum_creatinine), dist = "lognorm")
```

```{r}
lrtest(lognormal.28,lognormal.31)
```

Nenhum dos termos de interação apresentou significância estatística para o modelo.

\newpage

#### Etapa 6: Ajuste do modelo final

```{r}
lognormal.reduzida = survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine, dist = "lognorm")
summary(lognormal.reduzida)
```
\newpage

### Stepwise 


Nesta etapa, vamos utilizar uma função já implementada no R chamada `stepwiseCox()`, que serve para realizar a seleção de variáveis para modelos de sobrevivência.

\fontsize{6.6pt}{6.6pt}

```{r}
stepwiseCox(data = dados, s ~ age+anaemia+creatinine_phosphokinase+diabetes+ejection_fraction+high_blood_pressure+platelets+serum_creatinine+serum_sodium+sex+smoking, selection = "bidirection",sle = 0.05,sls = 0.05)
```


```{r}
lognormal.reduzida_step <- survreg(data = dados, s ~ age+ejection_fraction+serum_creatinine+high_blood_pressure, dist = "lognorm")
summary(lognormal.reduzida_step)
```


\newpage

## Analise de residuos

### Modelo Manual

```{r}
y = log(dados$tempo)
mip = lognormal.reduzida$linear.predictors
Smod = 1-pnorm((y-mip)/lognormal.reduzida$scale)
ei = (-log(Smod))


Kmew = survfit(Surv(ei,dados$censura)~1, conf.int = F)
te = Kmew$time
ste = Kmew$surv
sexp = exp(-te)

par(mfrow = c(1,1))

plot(ste,sexp, xlab = "S(ei): Kaplan-Meier", ylab = "S(ei): Exponencial Padrao")
plot(Kmew,conf.int = F, xlab = "Residuos de Cox-Snell", ylab = "Sobrevivencia estimada")
lines(te,sexp,lty=2,col=2)
legend(0.6,1.0,lty=c(1,2),c("Kaplan-Meier","Exponencial padrao"),cex=0.8, bty = "n")

```

### Modelo Stepwise

```{r}
y = log(dados$tempo)
mip = lognormal.reduzida_step$linear.predictors
Smod = 1-pnorm((y-mip)/lognormal.reduzida$scale)
ei = (-log(Smod))


Kmew = survfit(Surv(ei,dados$censura)~1, conf.int = F)
te = Kmew$time
ste = Kmew$surv
sexp = exp(-te)

par(mfrow = c(1,1))

plot(ste,sexp, xlab = "S(ei): Kaplan-Meier", ylab = "S(ei): Exponencial Padrao")
plot(Kmew,conf.int = F, xlab = "Residuos de Cox-Snell", ylab = "Sobrevivencia estimada")
lines(te,sexp,lty=2,col=2)
legend(0.6,1.0,lty=c(1,2),c("Kaplan-Meier","Exponencial padrao"),cex=0.8, bty = "n")

```


## Comparando os modelos finais

\fontsize{11pt}{11pt}

```{r}
manual = c(AIC(lognormal.reduzida),AICc(lognormal.reduzida),BIC(lognormal.reduzida))
```

```{r}
cat("Valores modelo manual:",manual)
```

```{r}
manual = c(AIC(lognormal.reduzida_step),AICc(lognormal.reduzida_step),BIC(lognormal.reduzida_step))
```

```{r}
cat("Valores modelo stepwise:",manual)
```

A análise do gráfico de resíduos de Cox-Snell sugere que ambos os modelos apresentaram um ajuste satisfatório até o ponto de resíduo 0.6. No entanto, no trecho final, a precisão diminuiu. Observando visualmente, nota-se que o modelo stepwise está mais próximo da curva no final em comparação ao modelo selecionado manualmente. Quanto às medidas AIC e BIC, estão muito próximas, indicando que ambos os modelos podem ser utilizados e interpretados. Com base na diferença gráfica, optaremos pelo modelo final do Stepwise.

# Conclusão

Ao interpretar o modelo final, observamos que os coeficientes das variáveis `high_blood_pressure`, `serum_creatinine` e `age` contribuem para a redução da chance de sobrevivência do indivíduo, ou seja, aumentam a probabilidade de ocorrência de uma falha cardíaca. Por outro lado, o coeficiente da variável `ejection_fraction` auxilia na diminuição da probabilidade de falha, aumentando assim a chance de sobrevivência.


# Referencia Bibliografica

COLOSIMO, E. A.; GIOLO, S. R. Análise de Sobrevivência Aplicada. Edgard Blucher, São Paulo, 2006.

CORDEIRO, G. M.; ORTEGA, E. M. M.; SILVA, G. O. Modelos de Regressão Estendidos em Análise de Sobrevivência. XII Escola de Modelos de Regressão, 2011.

KALBFLEISH, J. D.; PRENTICE, R. L. The Statistical Analysis of Failure Time Data. John Wiley & Sons, New York, 2002.

LAI, C. D.; XIE, M.; MURTHY, D. N. P. Modified Weibull model. IEEE Transactions on Reliability, v. 52, p. 33-37, 2003.

LAWLESS, J. F. Statistical Methods and Models for Lifetime Data. John Wiley & Sons, New York, 1982.

